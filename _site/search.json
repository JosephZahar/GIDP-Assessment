[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R skills assessment",
    "section": "",
    "text": "JosephZahar"
  },
  {
    "objectID": "index.html#basic-stats",
    "href": "index.html#basic-stats",
    "title": "R skills assessment",
    "section": "Basic Stats",
    "text": "Basic Stats\n\n1. Summary statistics of GDP per capita by region\n\nweighted_sd &lt;- function(x, pop) {\n  sqrt(weighted.mean((x - weighted.mean(x, pop, na.rm = TRUE))^2, \n                     pop, na.rm = TRUE))\n}\ngdp_sum &lt;- wdi[, .(\n  N = sum(!is.na(gdp)),\n  Mean = weighted_sd(gdp, pop),\n  SD = sqrt(weighted.mean((gdp - weighted.mean(gdp, pop, na.rm = TRUE))^2, \n                          pop, na.rm = TRUE)),\n  Min = min(gdp, na.rm = TRUE),\n  Max = max(gdp, na.rm = TRUE)\n), by = .(region, date)]\n\nsetorder(gdp_sum, region, date)\nsetnames(gdp_sum, \"date\", \"year\")\ncorrect_q1 &lt;- readr::read_rds(paste0(data_url, \"wdi_summ_out.Rds\"))\n# waldo::compare(correct_q1, gdp_sum)\ndatatable(gdp_sum, options = list(pageLength = 10))\n\n\n\n\n\n\n\n\n2. Aggregate Stats\n\nagg_stats &lt;- wdi[, .(\n  mean_lifeex = weighted.mean(lifeex, pop, na.rm = TRUE),\n  sd_lifeex = weighted_sd(lifeex, pop),\n  min_lifeex = min(lifeex, na.rm = TRUE),\n  max_lifeex = max(lifeex, na.rm = TRUE),\n  median_lifeex = weighted.median(lifeex, pop, na.rm = TRUE),\n  mean_gdp = weighted.mean(gdp, pop, na.rm = TRUE),\n  sd_gdp = weighted_sd(gdp, pop),\n  min_gdp = min(gdp, na.rm = TRUE),\n  max_gdp = max(gdp, na.rm = TRUE),\n  median_gdp = weighted.median(gdp, pop, na.rm = TRUE),\n  mean_pov_intl = weighted.mean(pov_intl, pop, na.rm = TRUE),\n  sd_pov_intl = weighted_sd(pov_intl),\n  min_pov_intl = min(pov_intl, na.rm = TRUE),\n  max_pov_intl = max(pov_intl, na.rm = TRUE),\n  median_pov_intl = weighted.median(pov_intl, pop, na.rm = TRUE),\n  pop = sum(pop, na.rm = TRUE)\n),\nby = .(region, date)\n]\n\nagg_stats &lt;- melt(agg_stats,\n  id.vars = c(\"region\", \"date\", \"pop\"),\n  measure.vars = list(\n    c(\"mean_lifeex\", \"sd_lifeex\", \"min_lifeex\", \"max_lifeex\", \"median_lifeex\"),\n    c(\"mean_gdp\", \"sd_gdp\", \"min_gdp\", \"max_gdp\", \"median_gdp\"),\n    c(\"mean_pov_intl\", \"sd_pov_intl\", \"min_pov_intl\", \"max_pov_intl\", \n      \"median_pov_intl\")\n  ),\n  variable.name = \"estimate\", value.name = c(\"lifeex\", \"gdp\", \"pov_intl\")\n)\n\nagg_stats[, estimate := factor(estimate, labels = c(\"mean\", \"sd\", \"min\", \"max\", \n                                                    \"median\"))]\nsetorder(agg_stats, estimate, region, date)\nagg_stats &lt;- agg_stats[, c(4, 1, 2, 3, 5, 6, 7)]\n\ncorrect_q2 &lt;- readr::read_rds(paste0(data_url, \"wdi_agg_out.Rds\"))\n# waldo::compare(correct_q2, agg_stats)\n\ndatatable(agg_stats, options = list(pageLength = 10))\n\n\n\n\n\n\n\n\n3. Find outliers\n\nis_outlier_cols &lt;- function(dt, col) {\n  new_col_ll &lt;- paste0(\"ll_\", col)\n  new_col_hl &lt;- paste0(\"hl_\", col)\n  mean_col &lt;- paste0(\"mean_\", col)\n  sd_col &lt;- paste0(\"sd_\", col)\n  dt[, (new_col_ll) := get(col) &lt; get(mean_col) - 2.5 * get(sd_col), \n     by = 1:nrow(dt)]\n  dt[, (new_col_hl) := get(col) &gt; get(mean_col) + 2.5 * get(sd_col), \n     by = 1:nrow(dt)]\n\n  return(dt)\n}\n\ntemp_dt &lt;- wdi[, .(\n  mean_lifeex = weighted.mean(lifeex, pop, na.rm = TRUE),\n  sd_lifeex = weighted_sd(lifeex, pop),\n  mean_gdp = weighted.mean(gdp, pop, na.rm = TRUE),\n  sd_gdp = weighted_sd(gdp, pop),\n  mean_gini = weighted.mean(gini, pop, na.rm = TRUE),\n  sd_gini = weighted_sd(gini, pop)\n), by = .(date)]\n\noutliers_dt &lt;- merge(y = temp_dt, x = wdi, by = c(\"date\"), all.x = TRUE)\nsetorder(outliers_dt, iso3c, date, -region)\n\nfor (col in c(\"lifeex\", \"gdp\", \"gini\")) {\n  outliers_dt &lt;- is_outlier_cols(outliers_dt, col)\n}\n\ncorrect_q3 &lt;- readr::read_rds(paste0(data_url, \"wdi_outliers_out.Rds\"))\noutliers_dt &lt;- outliers_dt[, colnames(correct_q3), with = FALSE]\n# waldo::compare(correct_q3, outliers_dt)\n\ndatatable(outliers_dt, options = list(pageLength = 10))\n\n\n\n\n\n\n\noutlier_cols &lt;- function(dt, col) {\n  new_col_ll &lt;- paste0(\"lo_ci_\", col)\n  new_col_hl &lt;- paste0(\"hi_ci_\", col)\n  mean_col &lt;- paste0(\"mean_\", col)\n  sd_col &lt;- paste0(\"sd_\", col)\n  dt[, (new_col_ll) := get(mean_col) - 2.5 * get(sd_col), by = 1:nrow(dt)]\n  dt[, (new_col_hl) := get(mean_col) + 2.5 * get(sd_col), by = 1:nrow(dt)]\n\n  return(dt)\n}\noutliers_dt_2 &lt;- unique(outlier_cols(outliers_dt, \"lifeex\"), \n                        by = c(\"date\", \"lo_ci_lifeex\", \"hi_ci_lifeex\"))\n\nggplot(data = outliers_dt, aes(x = date, y = lifeex)) +\n  geom_ribbon(data = outliers_dt_2, aes(x = date, ymin = lo_ci_lifeex, \n                                        ymax = hi_ci_lifeex), alpha = 0.2) +\n  geom_point(aes(color = region), size = 0.8) +\n  geom_line(aes(x = date, y = mean_lifeex), color = \"blue\", linewidth = 0.2) +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.5, 0.1),\n    legend.justification = c(0.5, 0),\n    legend.direction = \"horizontal\",\n    legend.title = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank()\n  )"
  },
  {
    "objectID": "index.html#simulated-data",
    "href": "index.html#simulated-data",
    "title": "R skills assessment",
    "section": "Simulated data",
    "text": "Simulated data\n\n4. Poverty measures\n\ncols &lt;- c(\"year\", \"pov_line\", \"headcount\", \"povgap\", \"povseverity\")\npov_dt &lt;- data.table(matrix(ncol = length(cols), nrow = 0))\nsetnames(pov_dt, cols)\n\nFGT &lt;- function(pov_line, year, dt) {\n  N &lt;- sum(dt$weight)\n  dt[, `:=`(FGTi = (pov_line - income) / pov_line)]\n  dt_subset &lt;- dt[income &lt;= pov_line]\n  FGT0 &lt;- sum(dt_subset$weight * dt_subset$FGTi^0) / N\n  FGT1 &lt;- sum(dt_subset$weight * dt_subset$FGTi^1) / N\n  FGT2 &lt;- sum(dt_subset$weight * dt_subset$FGTi^2) / N\n\n  new_data &lt;- data.table(year = year, pov_line = pov_line, headcount = FGT0, \n                         povgap = FGT1, povseverity = FGT2)\n  pov_dt &lt;&lt;- rbindlist(list(pov_dt, new_data), use.names = TRUE, fill = TRUE)\n}\n\nyear &lt;- 2001\nfor (dt in svy_sim) {\n  FGT(2.15, year, dt)\n  FGT(3.65, year, dt)\n  FGT(6.85, year, dt)\n  year &lt;- year + 1\n}\n\ncorrect_q4 &lt;- readr::read_rds(paste0(data_url, \"dt_pov_out.Rds\"))\n# waldo::compare(correct_q4, pov_dt)\ndatatable(pov_dt, options = list(pageLength = 10))\n\n\n\n\n\n\n\nggplot(data = pov_dt, aes(x = year, y = headcount, group = pov_line, \n                          color = as.factor(pov_line))) +\n  geom_line(linewidth = 0.5) +\n  geom_point(size = 0.8) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.direction = \"horizontal\",\n    legend.title = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank()\n  )\n\n\n\n\n\n\n5. Lorenz curve\n\ncols &lt;- c(\"welfare\", \"cum_welfare\", \"cum_population\", \"year\", \"bin\")\nlorenz_dt &lt;- data.table(matrix(ncol = length(cols), nrow = 0))\nsetnames(lorenz_dt, cols)\n\nLorenz &lt;- function(dt, year) {\n  dt &lt;- dt[order(dt$income), ]\n  dt$cum_pop &lt;- cumsum(dt$weight) / sum(dt$weight)\n  dt$cum_welfare &lt;- cumsum(dt$weight * dt$income) / sum(dt$weight * dt$income)\n  dt$welfare &lt;- cumsum(dt$weight * dt$income)\n\n\n  approx_points &lt;- approx(dt$cum_pop, dt$cum_welfare, n = 100)\n  income_val &lt;- sapply(approx_points$x, function(x) {\n    idx &lt;- which.min(abs(dt$cum_pop - x))\n    return(dt$income[idx])\n  })\n\n  new_data &lt;- data.frame(welfare = income_val, cum_welfare = approx_points$y, \n                    cum_population = approx_points$x, year = year, bin = 1:100)\n  lorenz_dt &lt;&lt;- rbindlist(list(lorenz_dt, new_data), use.names = TRUE, \n                          fill = TRUE)\n}\n\nyear &lt;- 2001\nfor (dt in svy_sim) {\n  Lorenz(dt, year)\n  year &lt;- year + 1\n}\n\ncorrect_q5 &lt;- readr::read_rds(paste0(data_url, \"dt_lorenz_out.Rds\"))\n# waldo::compare(correct_q4, pov_dt)\ndatatable(lorenz_dt, options = list(pageLength = 10))\n\n\n\n\n\n\n\nggplot(data = lorenz_dt, aes(x = cum_population, y = cum_welfare, group = year, \n                             color = as.factor(year))) +\n  geom_line(linewidth = 0.4) +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.1, 0.2),\n    legend.justification = c(0.5, 0),\n    legend.direction = \"vertical\",\n    legend.title = element_blank(),\n    legend.background = element_blank(),\n    legend.box.background = element_blank()\n  )\n\n\n\n\n\n\n6. Gini coefficient\n\ncols &lt;- c(\"year\", \"gini\")\ngini_dt &lt;- data.table(matrix(ncol = length(cols), nrow = 0))\nsetnames(gini_dt, cols)\n\nGini &lt;- function(dt, years) {\n  dt &lt;- dt[year == years]\n  setorder(dt, bin)\n  A &lt;- 0\n  for (i in 2:length(dt$cum_pop)) {\n    width &lt;- dt$cum_pop[i] - dt$cum_pop[i - 1]\n    height_avg &lt;- (dt$cum_welfare[i] + dt$cum_welfare[i - 1]) / 2\n    A &lt;- A + (width * height_avg)\n  }\n\n\n  gini_index &lt;- 1 - 2 * A\n  new_data &lt;- data.frame(year = years, gini = gini_index)\n  gini_dt &lt;&lt;- rbindlist(list(gini_dt, new_data), use.names = TRUE, fill = TRUE)\n}\n\nyear &lt;- 2001\nfor (i in 1:10) {\n  Gini(lorenz_dt, year)\n  year &lt;- year + 1\n}\n\ncorrect_q6 &lt;- readr::read_rds(paste0(data_url, \"dt_gini_out.Rds\"))\n# waldo::compare(correct_q6, gini_dt)\ndatatable(gini_dt, options = list(pageLength = 10))\n\n\n\n\n\n\n\nggplot(data = gini_dt, aes(x = year, y = gini)) +\n  geom_line(linewidth = 0.4) +\n  geom_point(size = 0.8) +\n  theme_minimal()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4"
  },
  {
    "objectID": "about.html#quarto",
    "href": "about.html#quarto",
    "title": "about",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4"
  },
  {
    "objectID": "dash.html",
    "href": "dash.html",
    "title": "Previous Work",
    "section": "",
    "text": "Below are links to some of the interactive dashboards I have created in the past year. Please note that each dashboard was designed and implemented from scratch using Python and was deployed online. Corresponding links to the code base for each repository are also provided. (For an optimal UI and format, Google Chrome is recommended).\n\n\nThis dashboard offers a comprehensive view designed to compare and analyze various definitions of surgical procedure durations. Its primary objective is to standardize and clarify the different time metrics used in surgical practices, ensuring a consistent understanding that can lead to enhanced patient care. The dashboard comprises three sections: a composition page, a comparative page, and a distribution page. These sections delve into analyzing each definition by hospital and surgeon. Please note that all data has been anonymized to maintain confidentiality.\nGitHub Repository\n\n\n\n\nThis open-source dashboard provides a comprehensive view into flight prices, specifically from United Airlines. With its user-friendly interface, the dashboard transforms raw flight data into captivating visuals and insightful charts, enabling users to discern hidden trends and gain a deeper understanding of the airline industry’s dynamics. The data is sourced using dynamic web scraping with Python’s beautiful soup and selenium. This method allows for the automated tracking of flight price fluctuations, ensuring the dashboard remains updated with the latest trends. Note that the data presented is notional in this instance, primarily for aesthetic purposes and to showcase the dashboard’s capabilities.\nGitHub Repository\nHow to use: Select a specific depart date (left side) and return date (right side) to narrow the search, or leave as All\n\n\n\n\nThis repository centers on the in-depth analysis of machine learning-labeled surgical instrument annotations that we received for various surgeries. Our primary objective is to employ data mining techniques to unearth patterns within this data, aiming to derive meaningful insights. The developed dashboard delves into individual surgeries, showcasing plots for each alongside their respective autocorrelation plots. These autocorrelation plots were subjected to k-means clustering to shed light on the underlying patterns and trends in instrument annotations across different surgical procedures. Note that the data presented is notional in this instance, primarily for aesthetic and confidentiality purposes. GitHub Repository How to use: Since the application is deployed on a free subscription, this dashboard may take some time to run due to the heavy operations running in the background. Otherwise the GitHub link provides a static view of the dashboard. \n\n\n\nAs part of the Google Isolated Sign Language Recognition Kaggle competition, which seeks to build an AI model that recognize ASL, I built this interactive dashboard that delves into ASL landmarks within a 2D space. Users can navigate through diverse ASL categories and IDs, honing in on specific landmarks to discern the hand gestures and movements tied to particular signs. This dashboard not only serves as an educational resource but also as a foundation for the development and refinement of ASL recognition systems.\nGitHub Repository\nHow to use: Select the Sign you wish to visualize —&gt; select the sequence id from the dropdown options —&gt; press Play ▶\n\nif interested, below is the strategy used to deploy and build my model for the competition, uing a TensorFlow Lite model to perform real-time American Sign Language (ASL) recognition from your webcam video:"
  },
  {
    "objectID": "dash.html#interactive-dashboards",
    "href": "dash.html#interactive-dashboards",
    "title": "Previous Work",
    "section": "",
    "text": "Below are links to some of the interactive dashboards I have created in the past year. Please note that each dashboard was designed and implemented from scratch using Python and was deployed online. Corresponding links to the code base for each repository are also provided. (For an optimal UI and format, Google Chrome is recommended).\n\n\nThis dashboard offers a comprehensive view designed to compare and analyze various definitions of surgical procedure durations. Its primary objective is to standardize and clarify the different time metrics used in surgical practices, ensuring a consistent understanding that can lead to enhanced patient care. The dashboard comprises three sections: a composition page, a comparative page, and a distribution page. These sections delve into analyzing each definition by hospital and surgeon. Please note that all data has been anonymized to maintain confidentiality.\nGitHub Repository\n\n\n\n\nThis open-source dashboard provides a comprehensive view into flight prices, specifically from United Airlines. With its user-friendly interface, the dashboard transforms raw flight data into captivating visuals and insightful charts, enabling users to discern hidden trends and gain a deeper understanding of the airline industry’s dynamics. The data is sourced using dynamic web scraping with Python’s beautiful soup and selenium. This method allows for the automated tracking of flight price fluctuations, ensuring the dashboard remains updated with the latest trends. Note that the data presented is notional in this instance, primarily for aesthetic purposes and to showcase the dashboard’s capabilities.\nGitHub Repository\nHow to use: Select a specific depart date (left side) and return date (right side) to narrow the search, or leave as All\n\n\n\n\nThis repository centers on the in-depth analysis of machine learning-labeled surgical instrument annotations that we received for various surgeries. Our primary objective is to employ data mining techniques to unearth patterns within this data, aiming to derive meaningful insights. The developed dashboard delves into individual surgeries, showcasing plots for each alongside their respective autocorrelation plots. These autocorrelation plots were subjected to k-means clustering to shed light on the underlying patterns and trends in instrument annotations across different surgical procedures. Note that the data presented is notional in this instance, primarily for aesthetic and confidentiality purposes. GitHub Repository How to use: Since the application is deployed on a free subscription, this dashboard may take some time to run due to the heavy operations running in the background. Otherwise the GitHub link provides a static view of the dashboard. \n\n\n\nAs part of the Google Isolated Sign Language Recognition Kaggle competition, which seeks to build an AI model that recognize ASL, I built this interactive dashboard that delves into ASL landmarks within a 2D space. Users can navigate through diverse ASL categories and IDs, honing in on specific landmarks to discern the hand gestures and movements tied to particular signs. This dashboard not only serves as an educational resource but also as a foundation for the development and refinement of ASL recognition systems.\nGitHub Repository\nHow to use: Select the Sign you wish to visualize —&gt; select the sequence id from the dropdown options —&gt; press Play ▶\n\nif interested, below is the strategy used to deploy and build my model for the competition, uing a TensorFlow Lite model to perform real-time American Sign Language (ASL) recognition from your webcam video:"
  }
]